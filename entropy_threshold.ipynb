{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import lesion\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "import unet\n",
    "import residual_unet\n",
    "import utils as util\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import jaccard_score as jsc\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor_segmap(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, segmap):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        # In this case, Channels is 1, so there is no need to swap since data is in HxW      \n",
    "  \n",
    "        segmap = np.array(segmap)\n",
    "        return torch.from_numpy(segmap) / 255\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "        transforms.RandomGrayscale(1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "seg_transform = ToTensor_segmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "dataset = lesion.LesionDataset(\"data\",folder_name = 'val',joint_transform=False,img_transform=image_transform, seg_transform=seg_transform,verbose = True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size,shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): res_unet_gate(\n",
       "    (MaxPool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (drop): GaussianDropout()\n",
       "    (preprocess): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (down1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (down2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (down3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (up1): upblock(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (upconv): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (up2): upblock(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (upconv): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (up3): upblock(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (upconv): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (gate1): gate(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (gate2): gate(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (gate3): gate(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (up4): upblock(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "      (upconv): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    )\n",
       "    (logits): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"gate_gaussian.pth\"\n",
    "model = torch.load(\"models/\" + file_name)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dropout(m):\n",
    "    if type(m) == nn.Dropout2d:\n",
    "        m.train()\n",
    "    if type(m) == residual_unet.GaussianDropout:\n",
    "        m.train()\n",
    "        \n",
    "#T is the number of times to sample to approximate the posterior predictive distribution\n",
    "#Assume model in eval mode\n",
    "\n",
    "def mc(loader,T,batch_size = 2,bayesian = True):\n",
    "    model.eval()\n",
    "    total = batch_size * len(loader)\n",
    "    if(bayesian):\n",
    "        print(\"Bayesian Mode\")\n",
    "        model.apply(apply_dropout)\n",
    "    mean = np.zeros((total, 2, 512, 1024))\n",
    "    var = np.zeros((total, 2, 512, 1024))\n",
    "    targets = np.zeros((total, 512, 1024))\n",
    "    for iteration in range(T):\n",
    "        for i,data in enumerate(loader):\n",
    "                image = data[0].to(device)\n",
    "                m = nn.Softmax2d()\n",
    "                output = model(image)\n",
    "                output = m(output)\n",
    "                output = output.detach().cpu().numpy()\n",
    "                segmap = data[1].numpy()\n",
    "                \n",
    "                mean[batch_size*i:batch_size*i + batch_size] += output\n",
    "                if(iteration==0):\n",
    "                    targets[batch_size*i:batch_size*i + batch_size] = segmap\n",
    "    mean = mean / T\n",
    "    entropy = -1*(mean[:,0,]*np.log(mean[:,0,]) + ((mean[:,1,])*np.log(mean[:,1,]))) # we only have two classes\n",
    "    return mean,entropy,targets\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "     \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Mode\n"
     ]
    }
   ],
   "source": [
    "mean,entropy,targets = mc(loader,5,bayesian=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95097451533598742"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence = mean.max(axis=1)\n",
    "avg_conf = 0\n",
    "for i in range(150):\n",
    "    avg_conf += confidence[i].mean()\n",
    "avg_conf/=150\n",
    "avg_conf #average pixel confidence of the val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1263282987004892"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_uncertainty = 0\n",
    "min_uncertainty = 100\n",
    "max_uncertainty = -1\n",
    "#150 images in test set\n",
    "for i in range(150):\n",
    "    avg_uncertainty += entropy[i].mean()\n",
    "    if(np.amin(entropy[i]) < min_uncertainty):\n",
    "        min_uncertainty = np.amin(entropy[i])\n",
    "    if(np.amax(entropy[i]) > max_uncertainty):\n",
    "        max_uncertainty = np.amax(entropy[i])\n",
    "avg_uncertainty/=150\n",
    "avg_uncertainty # avg uncertainty of the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Uncertainty: 0.6931471938503913  Min Uncertainty: 4.1248185617728174e-08 \n"
     ]
    }
   ],
   "source": [
    "print(\"Max Uncertainty: {}  Min Uncertainty: {} \".format(max_uncertainty,min_uncertainty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0693147565084\n"
     ]
    }
   ],
   "source": [
    "#t is a parameter, you can set this yourself\n",
    "t=0.1\n",
    "uncertainty_threshold = (min_uncertainty +t*(max_uncertainty - min_uncertainty))\n",
    "print(uncertainty_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = lesion.LesionDataset(\"data\",folder_name = 'test',joint_transform=False,img_transform=image_transform, seg_transform=seg_transform,verbose = True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size,shuffle=False, num_workers=4)\n",
    "total = len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Mode\n"
     ]
    }
   ],
   "source": [
    "mean,entropy,targets = mc(test_loader,5,bayesian=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm= entropy.copy() #Certainty map\n",
    "pred = mean.argmax(axis=1)# test prediction\n",
    "upper = np.nonzero(cm > avg_uncertainty) # Can uncertainty_threshold instead or avg_uncertainty\n",
    "lower = np.nonzero(cm <= avg_uncertainty)\n",
    "cm[upper] = 0\n",
    "cm[lower]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(pred,t):\n",
    "    jaccard = 0\n",
    "    for i in range(total):\n",
    "        jaccard += jsc(pred[i].reshape(-1),t[i].reshape(-1))\n",
    "    jaccard /= total\n",
    "    \n",
    "    return jaccard,np.count_nonzero(pred==t)/pred.reshape(-1).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard,accuracy = get_score(pred,targets)\n",
    "print(\"Jaccard Val:{}  Accuracy:{}\".format(jaccard,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "P(accurate|certain)\n",
    "'''\n",
    "def ac(gt,pred,cm):\n",
    "    #gt = groundtruth, pred = predictions, cm = certainty map\n",
    "    loc = np.nonzero(cm==1) # location of certain pixels\n",
    "    gt = gt[loc]\n",
    "    pred = pred[loc]\n",
    "    p = np.count_nonzero(gt == pred) # number of accurate pixels given they are certain\n",
    "    return p/np.count_nonzero(cm==1)\n",
    "'''\n",
    "p(uncertain|inaccurate)\n",
    "'''\n",
    "def uia(gt,pred,cm):\n",
    "    loc = np.nonzero(gt != pred) # location of inaccurate pixels\n",
    "    cm = cm[loc]\n",
    "    return np.count_nonzero(cm==0)/np.count_nonzero(gt!=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac = ac(targets,pred,cm) #prob of accurate given certain\n",
    "puia =uia(targets,pred,cm) #prob of uncertain given inaccurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prob of accurate given certain is:{} \".format(pac))\n",
    "print(\"Prob of uncertain given inaccurate is:{} \".format(puia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cm[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(targets[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
